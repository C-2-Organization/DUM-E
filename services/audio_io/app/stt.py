# services/audio_io/app/stt.py

import os
import time
import tempfile
from typing import Optional

import numpy as np
import sounddevice as sd
import scipy.io.wavfile as wav
from openai import OpenAI

from services.common.env_loader import load_env, get_env

load_env()

class StreamingSTT:
    """
    - wakewordê°€ ê°ì§€ëœ ì´í›„ì— í˜¸ì¶œë˜ëŠ” STT ëª¨ë“ˆ
    - ì‚¬ìš©ìê°€ ë§í•˜ëŠ” ë™ì•ˆ ê³„ì† ë…¹ìŒ
    - '5ì´ˆ ì´ìƒ' ìŒì„±ì´ ì—†ìœ¼ë©´ ë…¹ìŒì„ ì¢…ë£Œí•˜ê³  Whisperë¡œ ì „ì†¡
    """

    def __init__(
        self,
        samplerate: int = 16000,
        chunk_duration: float = 0.5,   # í•œ ë²ˆì— 0.5ì´ˆì”© ì½ê¸°
        silence_sec: float = 3.0,      # 5ì´ˆ ì´ìƒ ì¡°ìš©í•˜ë©´ ì¢…ë£Œ
        max_total_sec: float = 60.0,   # ì•ˆì „ì¥ì¹˜: ìµœëŒ€ 60ì´ˆê¹Œì§€ë§Œ ë“£ê¸°
        energy_threshold: float = 500, # ì´ ê°’ ì´ìƒì´ë©´ 'ì‚¬ëŒì´ ë§í•˜ëŠ” ì¤‘'ì´ë¼ê³  ê°„ì£¼
    ):
        api_key = get_env("OPENAI_API_KEY")
        self.client = OpenAI(api_key=api_key)
        self.samplerate = samplerate
        self.chunk_duration = chunk_duration
        self.silence_sec = silence_sec
        self.max_total_sec = max_total_sec
        self.energy_threshold = energy_threshold

    def _record_until_silence(self) -> np.ndarray:
        """
        sounddeviceë¡œ ë§ˆì´í¬ë¥¼ ì¡°ê¸ˆì”© ì½ìœ¼ë©´ì„œ
        - ìµœì´ˆë¡œ ìŒì„±ì´ ê°ì§€ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¸ë‹¤ê°€
        - ê·¸ ì´í›„ë¡œ 5ì´ˆ ì´ìƒ ì¡°ìš©í•˜ë©´ ì¢…ë£Œ
        """
        print("[STT] ğŸ™ ë…¹ìŒì„ ì‹œì‘í•©ë‹ˆë‹¤. ë§ì´ ëŠê¸°ë©´ ìë™ìœ¼ë¡œ ì¢…ë£Œë©ë‹ˆë‹¤.")
        num_samples_per_chunk = int(self.samplerate * self.chunk_duration)

        chunks: list[np.ndarray] = []
        start_time = time.time()
        last_voice_time = time.time()
        heard_voice = False

        while True:
            audio_block = sd.rec(
                num_samples_per_chunk,
                samplerate=self.samplerate,
                channels=1,
                dtype="int16",
            )
            sd.wait()

            block_energy = float(np.abs(audio_block).mean())

            chunks.append(audio_block.copy())

            now = time.time()

            if block_energy > self.energy_threshold:
                heard_voice = True
                last_voice_time = now

            if heard_voice and (now - last_voice_time) >= self.silence_sec:
                print("[STT] ğŸ¤« 3ì´ˆ ì´ìƒ ì¡°ìš©í•´ì„œ ë…¹ìŒì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            if (now - start_time) >= self.max_total_sec:
                print("[STT] â± ìµœëŒ€ ë…¹ìŒ ì‹œê°„ ì´ˆê³¼ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

        audio_all = np.concatenate(chunks, axis=0)
        return audio_all

    def listen_and_transcribe(self) -> str:
        """
        - ë§ˆì´í¬ì—ì„œ streamingìœ¼ë¡œ ìŒì„±ì„ ë°›ë‹¤ê°€
        - 5ì´ˆ ì´ìƒ ë¬´ìŒ êµ¬ê°„ì´ ë‚˜ì˜¤ë©´ ì¢…ë£Œ
        - Whisperë¡œ ì „ì†¡ í›„ í…ìŠ¤íŠ¸ ë°˜í™˜
        """
        audio_all = self._record_until_silence()

        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_wav:
            wav.write(temp_wav.name, self.samplerate, audio_all)
            temp_path = temp_wav.name

        print(f"[STT] ğŸ§ Whisperë¡œ ì „ì†¡ ì¤‘... ({temp_path})")

        with open(temp_path, "rb") as f:
            transcript = self.client.audio.transcriptions.create(
                model="whisper-1",
                file=f,
            )

        text = transcript.text
        print(f"[STT] âœ… ì¸ì‹ ê²°ê³¼: {text}")
        return text


if __name__ == "__main__":
    stt = StreamingSTT()
    msg = stt.listen_and_transcribe()
    print("ìµœì¢… í…ìŠ¤íŠ¸:", msg)

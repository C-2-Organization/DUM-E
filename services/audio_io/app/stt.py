# services/audio_io/app/stt.py

import time
import tempfile
from typing import Optional
import webrtcvad

import numpy as np
import sounddevice as sd
import scipy.io.wavfile as wav
from openai import OpenAI

from services.common.env_loader import load_env, get_env

load_env()


class StreamingSTT:
    """
    - wakewordê°€ ê°ì§€ëœ ì´í›„ì— í˜¸ì¶œë˜ëŠ” STT ëª¨ë“ˆ
    - ì‚¬ìš©ìê°€ ë§í•˜ëŠ” ë™ì•ˆ ê³„ì† ë…¹ìŒ
    - 'silence_sec ì´ìƒ' ìŒì„±ì´ ì—†ìœ¼ë©´ ë…¹ìŒì„ ì¢…ë£Œí•˜ê³  Whisperë¡œ ì „ì†¡
    - WebRTC VAD + Noise Gate + Adaptive Threshold ì ìš©
    """

    def __init__(
        self,
        samplerate: int = 16000,
        chunk_duration: float = 0.5,   # í•œ ë²ˆì— 0.5ì´ˆì”© ì½ê¸°
        silence_sec: float = 2.0,      # 2ì´ˆ ì´ìƒ ì¡°ìš©í•˜ë©´ ì¢…ë£Œ
        max_total_sec: float = 60.0,   # ì•ˆì „ì¥ì¹˜: ìµœëŒ€ 60ì´ˆê¹Œì§€ë§Œ ë“£ê¸°
        energy_threshold: float = 200, # ì´ ê°’ ì´ìƒì´ë©´ 'ì‚¬ëŒì´ ë§í•˜ëŠ” ì¤‘'ì´ë¼ê³  ê°„ì£¼
    ):
        api_key = get_env("OPENAI_API_KEY")
        if not api_key:
            raise RuntimeError("OPENAI_API_KEY not found in .env (StreamingSTT)")

        self.client = OpenAI(api_key=api_key)
        self.samplerate = samplerate
        self.chunk_duration = chunk_duration
        self.silence_sec = silence_sec
        self.max_total_sec = max_total_sec
        self.energy_threshold = energy_threshold

        # ğŸ”¥ ì¶”ê°€: WebRTC VAD + ambient ì—ë„ˆì§€ ì¶”ì •ìš©
        self.vad = webrtcvad.Vad(2)  # 0~3, í´ìˆ˜ë¡ ë” aggressive
        self.ambient_energy: float | None = None

        print(
            f"[STT] ğŸ”§ Initialized: samplerate={self.samplerate}, "
            f"chunk_duration={self.chunk_duration}, silence_sec={self.silence_sec}, "
            f"max_total_sec={self.max_total_sec}, energy_threshold={self.energy_threshold}"
        )

    def _record_until_silence(self) -> np.ndarray:
        """
        sounddeviceë¡œ ë§ˆì´í¬ë¥¼ ì¡°ê¸ˆì”© ì½ìœ¼ë©´ì„œ
        - ì´ˆê¸°ì—ëŠ” ambient noiseë¥¼ ì¸¡ì •í•´ì„œ adaptive threshold ì„¤ì •
        - WebRTC VAD + ì—ë„ˆì§€ë¥¼ ë™ì‹œì— ë§Œì¡±í•˜ëŠ” chunkë§Œ 'ë§í•˜ëŠ” ì¤‘'ìœ¼ë¡œ ê°„ì£¼
        - ê·¸ ì´í›„ë¡œ silence_sec ì´ìƒ ì¡°ìš©í•˜ë©´ ì¢…ë£Œ
        """
        print("[STT] ğŸ™ ë…¹ìŒì„ ì‹œì‘í•©ë‹ˆë‹¤. ë§ì´ ëŠê¸°ë©´ ìë™ìœ¼ë¡œ ì¢…ë£Œë©ë‹ˆë‹¤.")

        num_samples_per_chunk = int(self.samplerate * self.chunk_duration)
        vad_frame_ms = 20  # WebRTC VAD í—ˆìš©: 10 / 20 / 30 ms
        vad_frame_len = int(self.samplerate * vad_frame_ms / 1000)  # 20ms â†’ 320 ìƒ˜í”Œ

        chunks: list[np.ndarray] = []
        start_time = time.time()
        last_voice_time = time.time()
        heard_voice = False

        # ambient noise ì¶”ì •ìš©
        ambient_samples: list[float] = []
        ambient_collect_sec = 1.0  # ì²˜ìŒ 1ì´ˆ ì •ë„ëŠ” ì£¼ë³€ ì†ŒìŒ ê¸°ì¤€ ì¡ê¸°
        ambient_end_time = start_time + ambient_collect_sec

        while True:
            # 1) ë§ˆì´í¬ì—ì„œ chunk_duration ë§Œí¼ ì½ê¸°
            audio_block = sd.rec(
                num_samples_per_chunk,
                samplerate=self.samplerate,
                channels=1,
                dtype="int16",
            )
            sd.wait()

            # shape (N, 1) â†’ (N,)
            audio_block = audio_block.reshape(-1)

            # 2) ì´ chunkì˜ ì—ë„ˆì§€ ê³„ì‚°
            block_energy = float(np.abs(audio_block).mean())
            now = time.time()

            # --- ambient noise ì—…ë°ì´íŠ¸ (ì²˜ìŒ ì¼ì • ì‹œê°„ ë™ì•ˆ) ---
            if self.ambient_energy is None:
                ambient_samples.append(block_energy)
                if now >= ambient_end_time and ambient_samples:
                    self.ambient_energy = float(np.mean(ambient_samples))
                    print(f"[STT] ğŸŒ¡ ambient_energy ì¶”ì •: {self.ambient_energy:.2f}")
            ambient = self.ambient_energy or block_energy

            # Adaptive threshold: ì£¼ë³€ ì†ŒìŒì— ë¹„ë¡€í•´ì„œ ê°€ì¤‘
            adaptive_threshold = max(self.energy_threshold, ambient * 2.0)

            print(
                f"[STT] ğŸ”Š block_energy={block_energy:.2f}, "
                f"ambient={ambient:.2f}, adaptive_th={adaptive_threshold:.2f}"
            )

            # 3) ì „ì²´ ë…¹ìŒ ë²„í¼ì—ëŠ” ê³„ì† ì¶”ê°€ (ì•ë’¤ ì•½ê°„ì˜ ë¬´ìŒ í¬í•¨ìš©)
            chunks.append(audio_block.copy())

            # 4) ì´ chunk ì•ˆì—ì„œ VAD í”„ë ˆì„ ë‹¨ìœ„ë¡œ 'ë§í•˜ëŠ” êµ¬ê°„' ë¹„ìœ¨ ê³„ì‚°
            num_frames = len(audio_block) // vad_frame_len
            if num_frames <= 0:
                speech_ratio = 0.0
            else:
                speech_frames = 0
                for i in range(num_frames):
                    frame = audio_block[i * vad_frame_len : (i + 1) * vad_frame_len]
                    # WebRTC VADëŠ” 16bit PCM mono bytes ì…ë ¥
                    if self.vad.is_speech(frame.tobytes(), self.samplerate):
                        speech_frames += 1
                speech_ratio = speech_frames / float(num_frames)

            print(f"[STT] ğŸ—£ VAD speech_ratio={speech_ratio:.2f}")

            # 5) noise gate + VAD ë™ì‹œ ì¡°ê±´
            is_speech_block = (
                block_energy > adaptive_threshold and speech_ratio > 0.3
            )

            if is_speech_block:
                heard_voice = True
                last_voice_time = now

            # 6) ì‚¬ëŒì´ í•œ ë²ˆì´ë¼ë„ ë§í•œ ì´í›„ + silence_sec ì´ìƒ ì¡°ìš©í•˜ë©´ ì¢…ë£Œ
            if heard_voice and (now - last_voice_time) >= self.silence_sec:
                print(f"[STT] ğŸ¤« {self.silence_sec}ì´ˆ ì´ìƒ ì¡°ìš©í•´ì„œ ë…¹ìŒì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            # 7) ì•ˆì „ ì¥ì¹˜: ì „ì²´ ìµœëŒ€ ê¸¸ì´
            if (now - start_time) >= self.max_total_sec:
                print("[STT] â± ìµœëŒ€ ë…¹ìŒ ì‹œê°„ ì´ˆê³¼ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

        if not chunks:
            print("[STT] âš  ë…¹ìŒëœ chunkê°€ ì—†ìŠµë‹ˆë‹¤.")
            return np.zeros((0,), dtype=np.int16)

        audio_all = np.concatenate(chunks, axis=0)

        # --- ì „ì²´ êµ¬ê°„ì— ëŒ€í•´ 'ì§„ì§œ ë§ì´ ê±°ì˜ ì—†ìœ¼ë©´' ê·¸ëƒ¥ ë¹ˆ ë°°ì—´ ë°˜í™˜ (ì¡ìŒë§Œ ìˆëŠ” ê²½ìš°) ---
        total_frames = len(audio_all) // vad_frame_len
        if total_frames > 0:
            total_speech_frames = 0
            for i in range(total_frames):
                frame = audio_all[i * vad_frame_len : (i + 1) * vad_frame_len]
                if self.vad.is_speech(frame.tobytes(), self.samplerate):
                    total_speech_frames += 1
            total_speech_ratio = total_speech_frames / float(total_frames)
        else:
            total_speech_ratio = 0.0

        print(f"[STT] ğŸ“Š ì „ì²´ total_speech_ratio={total_speech_ratio:.2f}")

        if total_speech_ratio < 0.1:
            print("[STT] âš  ìŒì„± ë¹„ìœ¨ì´ ë„ˆë¬´ ë‚®ì•„ì„œ 'ë§ì´ ì—†ëŠ” ì¡ìŒ'ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.")
            return np.zeros((0,), dtype=np.int16)

        return audio_all

    def listen_and_transcribe(self) -> str:
        """
        - ë§ˆì´í¬ì—ì„œ streamingìœ¼ë¡œ ìŒì„±ì„ ë°›ë‹¤ê°€
        - silence_sec ì´ìƒ ë¬´ìŒ êµ¬ê°„ì´ ë‚˜ì˜¤ë©´ ì¢…ë£Œ
        - Whisperë¡œ ì „ì†¡ í›„ í…ìŠ¤íŠ¸ ë°˜í™˜
        - ì¡ìŒë§Œ ìˆì„ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
        """
        audio_all = self._record_until_silence()

        # ğŸ”¥ ìœ íš¨í•œ ìŒì„±ì´ ì—†ìœ¼ë©´ ë°”ë¡œ ë¹ˆ ë¬¸ìì—´ ë¦¬í„´
        if audio_all.size == 0:
            print("[STT] âš  ìœ íš¨í•œ ìŒì„±ì´ ì—†ì–´ì„œ ë¹ˆ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return ""

        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_wav:
            wav.write(temp_wav.name, self.samplerate, audio_all)
            temp_path = temp_wav.name

        print(f"[STT] ğŸ§ Whisperë¡œ ì „ì†¡ ì¤‘... ({temp_path})")

        # ğŸ”¥ Whisperì— prompt ì¶”ê°€: ë¡œë´‡ ëª…ë ¹ì–´ í™˜ê²½ì´ë¼ê³  íŒíŠ¸ ì£¼ê¸°
        with open(temp_path, "rb") as f:
            transcript = self.client.audio.transcriptions.create(
                model="whisper-1",
                file=f,
                prompt=(
                    "This audio comes from a robot command environment. "
                    "Ignore background noise, random conversations, and TV or music. "
                    "Only transcribe clear commands or questions addressed to the robot, "
                    "in Korean or English. If there is no clear speech, return an empty result."
                ),
            )

        text = transcript.text.strip()
        print(f"[STT] âœ… ì¸ì‹ ê²°ê³¼: {text!r}")
        return text

    # ğŸ”¥ main.py í˜¸í™˜ìš©: ê¸°ì¡´ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ ì¶”ê°€
    def transcribe_once(self) -> str:
        """
        main.pyì—ì„œ ì‚¬ìš©í•˜ëŠ” API.
        ë‚´ë¶€ì ìœ¼ë¡œ listen_and_transcribe()ë¥¼ ê·¸ëŒ€ë¡œ í˜¸ì¶œí•œë‹¤.
        """
        return self.listen_and_transcribe()


if __name__ == "__main__":
    stt = StreamingSTT()
    msg = stt.listen_and_transcribe()
    print("ìµœì¢… í…ìŠ¤íŠ¸:", msg)

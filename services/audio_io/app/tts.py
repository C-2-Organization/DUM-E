# services/audio_io/app/tts.py

import tempfile
from typing import Optional

import numpy as np

from services.audio_io.app.tts_base import BaseTTS

import sounddevice as sd
import scipy.io.wavfile as wav
from openai import OpenAI

from services.common.env_loader import load_env, get_env


class TTS(BaseTTS):
    """
    ë‹¨ìˆœ TTS ë˜í¼.
    - OpenAI audio.speech APIë¡œ í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜
    - ìƒì„±ëœ WAV íŒŒì¼ì„ sounddeviceë¡œ ì¬ìƒ
    """

    def __init__(
        self,
        model: str = "gpt-4o-mini-tts",
        voice: str = "verse",
        effect: str = "jarvis",  # none / jarvis
    ):
        load_env()
        api_key = get_env("OPENAI_API_KEY")
        if not api_key:
            raise RuntimeError("OPENAI_API_KEY not found in .env")

        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.voice = voice
        self.effect = effect  # ğŸ”¹ íš¨ê³¼ ëª¨ë“œ (ê¸°ë³¸: jarvis)

        print(
            f"[TTS] ğŸ”§ ê¸°ë³¸ ì„¤ì •: model={self.model}, voice={self.voice}, effect={self.effect}"
        )

    def _apply_jarvis_voice(self, data, sr):
        """
        ìë¹„ìŠ¤/ë¡œë´‡ ë¶„ìœ„ê¸° í•„í„° (ê°•í™” ë²„ì „):
        - ì €ì—­ì„ ë” ê°•í•˜ê²Œ ì˜¬ë ¤ì„œ ë¬µì§í•œ í†¤
        - ê³ ì—­ì„ ë” ë§ì´ ê¹ì•„ì„œ ë©”íƒˆë¦­Â·í†µì‹  ëŠë‚Œ
        - ê°€ë²¼ìš´ bit-crushë¡œ ë””ì§€í„¸ ëŠë‚Œ ì¶”ê°€
        - chorus/delayë¥¼ ë” í‚¤ì›Œì„œ 'AI ë³´ì´ìŠ¤' ìŠ¤íƒ€ì¼ ê°•í™”
        """
        # float32ë¡œ ë³€í™˜
        x = data.astype(np.float32)

        # ---------------------------
        # 1) FFT ê¸°ë°˜ EQ
        # ---------------------------
        X = np.fft.rfft(x, axis=0)
        n = X.shape[0]

        low = int(n * 0.05)   # ì €ì—­
        mid = int(n * 0.25)   # ì¤‘ì—­
        high = int(n * 0.55)  # ê³ ì—­ ì‹œì‘ êµ¬ê°„ (ì´ì „ë³´ë‹¤ ë” ë‚®ì€ ì§€ì ë¶€í„° ê¹ê¸°)

        # ì €ì—­ 40% ì¦ê°€ â†’ ë” ë¬´ê±°ìš´ ëŠë‚Œ
        X[:low] *= 1.2

        # ì¤‘ì—­ ì•½ê°„ ë³´ì • â†’ ëª…ë£Œë„ ìœ ì§€
        X[low:mid] *= 1.10

        # ê³ ì—­ì€ ì ˆë°˜ ìˆ˜ì¤€ê¹Œì§€ ê°ì‡„ â†’ ì‹œìŠ¤í…€/ì „í™”ê¸° ê°™ì€ ëŠë‚Œ
        X[high:] *= 0.65

        y = np.fft.irfft(X, n=data.shape[0], axis=0)

        # ---------------------------
        # 2) soft clipping + ê°€ë²¼ìš´ bit-crush
        # ---------------------------
        y = y / 32768.0

        # ì‚´ì§ ì„¸ê²Œ íƒœë‹í•´ì„œ ì„ ëª…ë„ í™•ë³´
        y = np.tanh(y * 1.4)

        # bit depthë¥¼ ì•½ê°„ ì¤„ì—¬ì„œ ë””ì§€í„¸ìŠ¤ëŸ¬ìš´ ì§ˆê° ì¶”ê°€
        # (ë„ˆë¬´ ì‹¬í•˜ë©´ ì§€ì§€ì§ê±°ë¦¬ë‹ˆ 256~512 ì •ë„ë¡œë§Œ ì¡°ì •)
        levels = 512.0
        y = np.round(y * levels) / levels

        y = y * 32768.0

        # ---------------------------
        # 3) metallic chorus / short delays
        # ---------------------------
        num_samples = y.shape[0]
        y_mix = y.copy()

        # ë” ìë¹„ìŠ¤ìŠ¤ëŸ½ê²Œ: ì§§ì€ ë”œë ˆì´ë¥¼ ì—¬ëŸ¬ ê°œ ì„ìŒ
        delays_ms = [4, 9, 13]      # ë°€ë¦¬ì´ˆ ë‹¨ìœ„ ë”œë ˆì´
        gains = [0.18, 0.12, 0.08]  # ê° ë”œë ˆì´ ë³¼ë¥¨ ë¹„ìœ¨

        for d_ms, g in zip(delays_ms, gains):
            d_samples = int(sr * d_ms / 1000.0)
            if 0 < d_samples < num_samples:
                delayed = np.zeros_like(y_mix)
                delayed[d_samples:] = y[:-d_samples] * g
                y_mix += delayed

        # ---------------------------
        # 4) ìµœì¢… ë³¼ë¥¨ ì¡°ì • + í´ë¦¬í•‘
        # ---------------------------
        y_mix *= 0.85

        return np.clip(y_mix, -32768, 32767).astype(np.int16)

    def set_voice(self, voice: str):
        """
        ëŸ°íƒ€ì„ì— ë³´ì´ìŠ¤ë¥¼ ë°”ê¾¸ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©.
        ì˜ˆ: tts.set_voice("alloy"), tts.set_voice("onyx")
        """
        print(f"[TTS] ğŸ”„ voice ë³€ê²½: {self.voice} -> {voice}")
        self.voice = voice

    def speak(self, text: str) -> Optional[str]:
        """
        ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ TTSë¡œ ì¬ìƒ.
        - textê°€ ë¹„ì–´ ìˆìœ¼ë©´ ì•„ë¬´ ê²ƒë„ í•˜ì§€ ì•ŠìŒ
        - tmp wav íŒŒì¼ ê²½ë¡œë¥¼ ë¦¬í„´ (ë””ë²„ê¹…/ë¡œê·¸ìš©)
        """
        text = text.strip()
        if not text:
            print("[TTS] ë¹ˆ í…ìŠ¤íŠ¸ë¼ì„œ ì¬ìƒì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return None

        print(
            f"[TTS] â–¶ TTS ì‹œì‘ (len={len(text)} chars, voice={self.voice}, "
            f"model={self.model}, effect={self.effect})"
        )

        # ì„ì‹œ wav íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
            tmp_path = tmp.name

        # OpenAI TTS í˜¸ì¶œ
        with self.client.audio.speech.with_streaming_response.create(
            model=self.model,
            voice=self.voice,
            input=text,
            response_format="wav",
        ) as response:
            response.stream_to_file(tmp_path)

        # WAV ë¡œë“œ
        sr, data = wav.read(tmp_path)

        # ìë¹„ìŠ¤ íš¨ê³¼ ì ìš©
        if self.effect == "jarvis":
            data = self._apply_jarvis_voice(data, sr)

        # ğŸ”¹ ì¬ìƒ ì†ë„ ì¡°ì ˆ (ìë¹„ìŠ¤ ëª¨ë“œì¼ ë•Œë§Œ ì‚´ì§ ëŠë¦¬ê²Œ)
        if self.effect == "jarvis":
            speed_factor = 0.95  # 0.85 = 15% ëŠë¦¬ê²Œ (0.7~0.9 ì‚¬ì´ì—ì„œ ì·¨í–¥ëŒ€ë¡œ ì¡°ì • ê°€ëŠ¥)
            playback_sr = int(sr * speed_factor)
        else:
            playback_sr = sr

        print(
            f"[TTS] ì¬ìƒ sample_rate={playback_sr}, shape={data.shape}, dtype={data.dtype}"
        )
        sd.play(data, playback_sr)
        sd.wait()

        print("[TTS] âœ… ì¬ìƒ ì™„ë£Œ")
        return tmp_path

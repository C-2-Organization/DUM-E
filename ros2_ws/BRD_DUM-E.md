# Dum-E 프로젝트 BRD (Business Requirements Document)

## 문서 정보

- **문서 버전**: 1.0  
- **작성일**: 2025-12-09  
- **문서 상태**: Draft  
- **승인자**: [승인자명]  
- **배포 대상**: 프로젝트 팀원, 지도교수, 관련 이해관계자  

---

## 1. 프로젝트 개요

### 1.1 프로젝트 기본 정보

**프로젝트 명**: Dum-E (협동 로봇 데모 시스템)  

**프로젝트 기간**: [시작일] ~ [종료일]  

**프로젝트 예산**: [예산 범위 또는 N/A]

### 1.2 프로젝트 목적

사람 주변에서 안전하게 작업을 수행하는 **협동 로봇(Collaborative Robot) 데모 시스템**을 구현하여, 로봇 제어·Vision·센서 통합·UI까지 아우르는 **실시간 인터랙티브 로봇 시스템**을 시연한다.

본 프로젝트는 단순 반복 동작이 아닌, **사람과 환경을 인식하고 상황에 맞춰 행동을 변화시키는 지능형 협동 로봇**의 가능성을 입증하는 것을 목표로 한다.

### 1.3 주요 결과물

| 결과물 | 설명 | 형식 |
| --- | --- | --- |
| **실제 로봇 데모** | 시나리오 기반 실시간 시연 | 현장 데모 + 영상 |
| **시스템 설계 문서** | 아키텍처, 데이터 플로우, 하드웨어 구성도 | PDF/Notion |
| **소스코드** | ROS2 기반 제어·센서·Vision 통합 코드 | GitHub Repository |
| **발표 자료** | 프로젝트 개요, 기술 설명, 데모 결과 | PPT + Notion |
| **사용자 매뉴얼** | 시스템 설치, 실행, 트러블슈팅 가이드 | PDF/Markdown |

---

## 2. 배경 및 문제 정의

### 2.1 현재 상황

- 기존 로봇 데모는 **사전 프로그래밍된 단순 반복 동작** 수준에 머물러 있음  
- 사람의 존재나 환경 변화를 인식하지 못하고 **고정된 루틴만 수행**  
- 협동 로봇의 핵심인 **"사람과의 안전한 협업"** 시나리오가 부재  
- 센서·Vision·제어 시스템이 **분리되어 있어 통합 시연이 어려움**

### 2.2 문제점

| 문제 | 영향 |
| --- | --- |
| 환경 인식 부재 | 사람이나 장애물이 있어도 로봇이 동작을 멈추지 않음 |
| 단조로운 시연 | 관객에게 "이미 본 것 같은" 인상, 임팩트 부족 |
| 통합 부족 | 센서·Vision·제어가 각각 따로 동작, 전체 시스템으로서 완성도 낮음 |
| 안전성 미비 | 비상 상황 대응 메커니즘 부족 |

### 2.3 Dum-E를 통한 해결 방향

✅ **사람/환경 인식 기반 반응형 로봇**: RealSense, 카메라를 활용한 실시간 인식  

✅ **상황별 동작 변화**: 사람 접근 시 속도 감속, 장애물 회피, 작업 중단 등  

✅ **통합 시스템 아키텍처**: ROS2 기반으로 센서·Vision·제어·UI를 하나의 파이프라인으로 연결  

✅ **안전 제어 로직**: 비상 정지, 안전 영역(Safety Zone) 설정, 실시간 모니터링  

---

## 3. 프로젝트 목표 및 성공 기준

### 3.1 정량적 목표

| 목표 항목 | 목표 값 | 측정 방법 |
| --- | --- | --- |
| 데모 시나리오 안정성 | 3회 이상 연속 성공 | 실제 시연 테스트 |
| 센서/카메라 인식률 | 80% 이상 | 테스트 케이스 기반 측정 |
| 비상 정지 반응 시간 | 0.5초 이내 | 로그 분석 |
| 시스템 초기화 시간 | 30초 이내 | 부팅~대기 상태 도달 시간 |
| 코드 문서화율 | 주요 모듈 80% 이상 | Docstring, README 작성률 |

### 3.2 정성적 목표

- 🎯 **임팩트**: "실제 산업 현장에 응용 가능하겠다"는 평가를 받을 수 있는 완성도  
- 🎯 **재사용성**: 포트폴리오·후속 연구에 활용 가능한 코드 및 문서 품질  
- 🎯 **이해도**: 비전공자도 시스템 동작 원리를 이해할 수 있는 발표 자료  
- 🎯 **안정성**: 데모 중 예기치 않은 중단 없이 안정적으로 동작  

### 3.3 비기능 요구사항

- **안전성**: 사람 접근 시 자동 감속/정지  
- **확장성**: 추가 센서·로봇 연동 가능한 모듈화 구조  
- **유지보수성**: 명확한 코드 주석, 로그 시스템, 에러 핸들링  
- **사용성**: 간단한 명령어로 시스템 시작/종료 가능  

---

## 4. 프로젝트 범위 (Scope)

### 4.1 In Scope (포함 사항)

#### 4.1.1 하드웨어 통합

- ✅ M0609 협동 로봇 기본 제어 (위치 제어, 속도 제어)  
- ✅ RealSense 또는 카메라를 통한 실시간 Vision 처리  
- ✅ 센서 데이터 수집 (거리, 깊이, 객체 인식)  

#### 4.1.2 소프트웨어 개발

- ✅ ROS2 기반 노드 아키텍처 설계  
- ✅ 센서 데이터 → 제어 로직 → 로봇 동작의 파이프라인 구현  
- ✅ 상태 머신(State Machine) 기반 시나리오 관리  
- ✅ 비상 정지 및 안전 제어 로직  

#### 4.1.3 시나리오 구현

- ✅ 시나리오 1: **물체 픽앤플레이스 (Pick & Place)**  
  - 사람이 물체를 지정하면 로봇이 인식하여 이동  
- ✅ 시나리오 2: **사람-로봇 협업 작업**  
  - 사람 접근 시 속도 조절, 안전 거리 유지  

#### 4.1.4 모니터링 및 UI

- ✅ 실시간 로그 출력 (터미널 기반)  
- ✅ 간단한 상태 모니터링 UI (웹 대시보드 또는 RViz)  

#### 4.1.5 문서 및 발표

- ✅ 시스템 아키텍처 다이어그램  
- ✅ 사용자 매뉴얼 (설치, 실행, 트러블슈팅)  
- ✅ 발표 자료 (PPT + Notion)  
- ✅ 데모 영상 제작  

### 4.2 Out of Scope (제외 사항)

- ❌ **멀티 로봇 시스템**: 복수의 로봇 또는 AGV 통합은 제외  
- ❌ **상용 인증**: ISO, CE 등 산업용 안전 인증은 포함하지 않음  
- ❌ **24/7 운영**: 연속 운전 시스템이 아닌 데모/연구용 프로토타입  
- ❌ **완전 자율 학습**: 딥러닝 모델 학습 및 배포는 최소화 (기존 모델 활용)  
- ❌ **클라우드 연동**: 외부 서버와의 데이터 통신은 포함하지 않음  

### 4.3 향후 확장 가능성 (Future Work)

- 🔮 멀티 로봇 협업 시나리오  
- 🔮 강화학습 기반 동작 최적화  
- 🔮 원격 모니터링 및 제어 시스템  
- 🔮 실제 산업 현장 파일럿 테스트  

---

## 5. 이해관계자 (Stakeholders)

### 5.1 프로젝트 팀 구성

| 역할 | 책임 | 담당자 |
| --- | --- | --- |
| **팀장 (PL)** | 일정 관리, 요구사항 정의, 품질 관리, 대외 커뮤니케이션 | [이름] |
| **로봇 제어 담당** | 로봇 API 연동, 모션 플래닝, 제어 알고리즘 | [이름] |
| **Vision/센서 담당** | 카메라/센서 데이터 처리, 객체 인식, 데이터 전처리 | [이름] |
| **시스템 통합 담당** | ROS2 노드 설계, 통신 아키텍처, 전체 파이프라인 통합 | [이름] |
| **문서/발표 담당** | 기술 문서 작성, 발표 자료 제작, 사용자 매뉴얼 작성 | [이름] |

### 5.2 외부 이해관계자

| 이해관계자 | 관심사 | 참여 방식 |
| --- | --- | --- |
| **지도교수/멘토** | 기술적 타당성, 학술적 가치 | 정기 미팅, 중간 검토 |
| **심사위원** | 프로젝트 완성도, 혁신성 | 최종 발표 평가 |
| **청중/참관자** | 데모 임팩트, 이해 가능성 | 시연 참관, 질의응답 |
| **장비 관리자** | 로봇 및 센서 안전 사용 | 장비 대여, 안전 교육 |

---

## 6. 주요 비즈니스 요구사항 (Business Requirements)

### 6.1 기능 요구사항

| ID | 요구사항 | 우선순위 | 비고 |
| --- | --- | --- | --- |
| **BR-01** | 사람/환경 인식 정보를 기반으로 로봇 동작이 실시간으로 변화해야 한다 | High | Vision/센서 통합 필수 |
| **BR-02** | 데모 시나리오는 일관된 스토리라인을 가져야 한다 (예: 작업자 보조, 정리 로봇) | High | 시나리오 문서화 필요 |
| **BR-03** | 시스템은 명확한 상태 전이를 가져야 한다 (초기화 → 대기 → 인식 → 작업 → 종료) | High | State Machine 구현 |
| **BR-04** | 로봇은 비상 상황에서 0.5초 이내에 정지할 수 있어야 한다 | Critical | 안전 최우선 |
| **BR-05** | 시스템은 비전문가도 쉽게 시작/정지/재시작할 수 있어야 한다 | Medium | CLI 또는 GUI 제공 |
| **BR-06** | 시스템 상태(대기, 작업 중, 에러 등)를 실시간으로 표시해야 한다 | Medium | LED, 터미널, UI 중 택 |
| **BR-07** | 코드와 문서는 재사용 및 확장 가능하도록 모듈화되어야 한다 | High | 후속 프로젝트 고려 |
| **BR-08** | 센서 데이터는 ROS2 토픽으로 퍼블리시되어 다른 노드에서 구독 가능해야 한다 | High | 표준 인터페이스 |

### 6.2 비기능 요구사항

| ID | 요구사항 | 기준 | 측정 방법 |
| --- | --- | --- | --- |
| **BR-09** | 시스템은 3회 연속 데모에서 에러 없이 동작해야 한다 | 성공률 100% | 실제 테스트 |
| **BR-10** | 센서 인식 정확도는 데모 환경에서 80% 이상이어야 한다 | 정확도 ≥ 80% | 테스트 데이터셋 |
| **BR-11** | 로봇 제어 주기는 최소 10Hz 이상이어야 한다 | 주기 ≤ 100ms | 로그 분석 |
| **BR-12** | 시스템 부팅 후 30초 이내에 대기 상태에 진입해야 한다 | 초기화 시간 ≤ 30s | 타이머 측정 |

---

## 7. 제약사항 (Constraints)

### 7.1 시간 제약

- 📅 **프로젝트 마감일**: [구체적 날짜] (예: 2025년 2월 28일)  
- 📅 **중간 발표**: [날짜]  
- 📅 **최종 발표**: [날짜]  
- ⏰ **주간 가용 시간**: 팀원별 약 [X]시간  

### 7.2 장비 제약

- 🤖 **로봇 사용 가능 시간**: 주 [X]시간 (연구실/랩 일정에 따름)  
- 🤖 **센서 대여 기간**: [시작일] ~ [종료일]  
- 🤖 **테스트 공간**: [장소명], 면적 [X]㎡  

### 7.3 기술 제약

- 💻 **개발 환경**: Ubuntu 22.04, ROS2 Humble  
- 💻 **프로그래밍 언어**: Python 3.10+, C++ (필요 시)  
- 💻 **로봇 SDK**: [M0609 API 버전]  
- 💻 **Vision 라이브러리**: OpenCV, RealSense SDK  

### 7.4 예산 제약

- 💰 **하드웨어 구매 예산**: [금액 또는 N/A]  
- 💰 **소프트웨어 라이선스**: 오픈소스 우선 사용  

---

## 8. 위험 요소 및 대응 전략 (Risks & Mitigation)

| 위험 요소 | 가능성 | 영향도 | 대응 전략 |
| --- | --- | --- | --- |
| **센서 인식 불안정** | High | High | 조명 조건 표준화, Fallback 로직(센서 실패 시 수동 모드), 다중 센서 Fusion |
| **로봇 충돌/안전사고** | Medium | Critical | 안전 영역 설정, 비상 정지 버튼 2중화, 사전 위험 평가 수행 |
| **팀원 일정 불일치** | Medium | Medium | 주간 동기화 미팅, 태스크 병렬화, 문서화 철저히 |
| **ROS2 학습 곡선** | Medium | Medium | 초기 2주 교육 기간, 튜토리얼 공유, 외부 멘토링 |
| **하드웨어 고장** | Low | High | 백업 장비 확보, 시뮬레이터 병행 개발, 보험 가입 검토 |
| **데모 당일 환경 변화** | Medium | High | 현장 사전 리허설, 환경 변수 체크리스트, 예비 시나리오 준비 |

---

## 9. 가정 사항 (Assumptions)

- ✔️ 프로젝트 기간 동안 로봇 및 센서 장비를 안정적으로 사용할 수 있다  
- ✔️ 팀원 모두 ROS2 기초 지식을 습득할 수 있다  
- ✔️ 데모 환경(조명, 공간)을 일정 수준으로 통제할 수 있다  
- ✔️ 지도교수/멘토로부터 주 1회 이상 피드백을 받을 수 있다  
- ✔️ 오픈소스 라이브러리 사용에 법적 제약이 없다  

---

## 10. 프로젝트 일정 개요 (High-Level Timeline)

| 단계 | 기간 | 주요 산출물 |
| --- | --- | --- |
| **1. 요구사항 정의 및 설계** | Week 1-2 | BRD, SRD, 아키텍처 설계서 |
| **2. 개발 환경 구축** | Week 2-3 | ROS2 환경, 로봇 API 연동 |
| **3. 센서/Vision 통합** | Week 3-5 | 센서 데이터 파이프라인, 객체 인식 |
| **4. 로봇 제어 구현** | Week 4-6 | 모션 플래닝, 안전 제어 |
| **5. 시스템 통합** | Week 6-7 | 전체 파이프라인 연결, State Machine |
| **6. 시나리오 개발** | Week 7-9 | 데모 시나리오 1, 2 구현 |
| **7. 테스트 및 디버깅** | Week 9-11 | 안정성 테스트, 버그 수정 |
| **8. 문서화 및 발표 준비** | Week 11-12 | 매뉴얼, PPT, 데모 영상 |
| **9. 최종 발표 및 시연** | Week 12 | 최종 발표, 데모 |

---

## 11. 승인 및 변경 관리

### 11.1 문서 승인

| 역할 | 이름 | 서명 | 날짜 |
| --- | --- | --- | --- |
| 작성자 | [이름] |  |  |
| 검토자 (팀장) | [이름] |  |  |
| 승인자 (지도교수) | [이름] |  |  |

### 11.2 변경 이력

| 버전 | 날짜 | 작성자 | 변경 내용 |
| --- | --- | --- | --- |
| 0.1 | 2025-12-09 | [이름] | 초안 작성 |
| 1.0 | 2025-12-09 | [이름] | BRD 1차 완성 |

### 11.3 변경 요청 프로세스

1. **변경 요청서 제출** → 팀장에게 이메일/이슈 트래커  
2. **영향도 분석** → 일정, 예산, 기술적 영향 검토  
3. **승인/반려** → 팀 미팅에서 결정  
4. **문서 업데이트** → 버전 관리  

---

## 12. 부록

### 12.1 용어 정의

- **협동 로봇 (Collaborative Robot, Cobot)**: 사람과 같은 공간에서 안전하게 작업하도록 설계된 로봇  
- **ROS2**: Robot Operating System 2, 로봇 소프트웨어 개발을 위한 미들웨어  
- **RealSense**: Intel의 깊이 카메라 제품군  
- **Pick & Place**: 물체를 집어서 다른 위치로 옮기는 작업  

### 12.2 참고 문헌

- ROS2 공식 문서: https://docs.ros.org/  
- Intel RealSense SDK: https://github.com/IntelRealSense  
- [로봇 제조사 매뉴얼]  

### 12.3 첨부 파일

- [ ] 시스템 아키텍처 초안  
- [ ] 하드웨어 스펙 시트  
- [ ] 예상 예산안  
